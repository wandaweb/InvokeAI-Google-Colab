
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Invoke AI Notebook\n",
        "\n",
        "Tested: Generating images with the SDXL base model and refiner.\n",
        "\n",
        "Broken: Adding custom checkpoints."
      ],
      "metadata": {
        "id": "D4TNDJdRpPN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvlaChZ3KLOm"
      },
      "outputs": [],
      "source": [
        "#@markdown # Installing InvokeAI\n",
        "\n",
        "#@markdown Use Google Drive to store models (uses about 13 GB). Uncheck this if you don't have enough space in your Drive.\n",
        "useGoogleDrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "googleDriveModelsFolder = '/stablemodels' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown This step usually takes about 5 minutes.\n",
        "import os\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "if useGoogleDrive:\n",
        "  drive.mount('/content/drive')\n",
        "  if not googleDriveModelsFolder.startswith('/'):\n",
        "    googleDriveModelsFolder = '/' + googleDriveModelsFolder\n",
        "  modelsPath = \"/content/drive/MyDrive\"+googleDriveModelsFolder\n",
        "  if not modelsPath.endswith(\"/\"):\n",
        "   modelsPath = modelsPath + \"/\"\n",
        "\n",
        "env = os.environ.copy()\n",
        "\n",
        "!pip install 'InvokeAI[xformers]' --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyElvKh3KVca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984ffe04-38d7-4ee3-fdb9-e5a5bb8cd3fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args='invokeai-configure --root_dir /content/invokeai --yes --default_only --skip-sd-weights', returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#@markdown # Downloading Models\n",
        "\n",
        "!mkdir /content/invokeai\n",
        "!mkdir /content/invokeai/configs\n",
        "\n",
        "#@markdown Download only the default model in initial configuration.\n",
        "#@markdown Checking this prevents running out of space in Colab.\n",
        "\n",
        "defaultOnly = True #@param {type:\"boolean\"}\n",
        "skipWeights = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown This step usually takes about 2 minutes with only the default model and no weights.\n",
        "\n",
        "#@markdown You can ignore \"File exists\" warnings in the output.\n",
        "\n",
        "cmd = 'invokeai-configure --root_dir /content/invokeai --yes'\n",
        "\n",
        "if defaultOnly:\n",
        "  cmd += ' --default_only'\n",
        "\n",
        "if skipWeights:\n",
        "  cmd += ' --skip-sd-weights'\n",
        "\n",
        "subprocess.run(cmd, shell=True, env=env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqTip65KKYBZ"
      },
      "outputs": [],
      "source": [
        "#@markdown Adding the SDXL Base Model\n",
        "\n",
        "#@markdown Installing SDXL base took about 20 minutes initially, but it's finished instantly\n",
        "#@markdown in subsequent runs if Google Drive is enabled. You can execute the first run in a runtime\n",
        "#@markdown without a GPU to save the model to Google Drive without spending GPU time.\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "# Install the SDXL base model\n",
        "def installSdxl(env):\n",
        "  installCmd = 'invokeai-model-install --add \"stabilityai/stable-diffusion-xl-base-1.0\" --root_dir /content/invokeai'\n",
        "  subprocess.run(installCmd, shell=True, env=env)\n",
        "\n",
        "sdxlBaseSubfolderName = '/stable-diffusion-xl-base-1-0'\n",
        "\n",
        "if useGoogleDrive:\n",
        "  alreadyInstalled = True\n",
        "\n",
        "  driveSdxlMainFolder = modelsPath + \"sdxl/main\"\n",
        "  if not path.exists(driveSdxlMainFolder):\n",
        "    os.makedirs(driveSdxlMainFolder, exist_ok=True)\n",
        "    alreadyInstalled = False\n",
        "\n",
        "  localModelsSdxlFolder = \"/content/invokeai/models/sdxl/\"\n",
        "  localSdxlMainFolder = localModelsSdxlFolder + \"main\"\n",
        "\n",
        "  subprocess.run('rm -rf ' + localModelsSdxlFolder, shell=True, env=env)\n",
        "  subprocess.run('rmdir ' + localModelsSdxlFolder, shell=True, env=env)\n",
        "\n",
        "  if not alreadyInstalled:\n",
        "    if not path.exists(localModelsSdxlFolder):\n",
        "      os.makedirs(localModelsSdxlFolder, exist_ok=True)\n",
        "    subprocess.run('ln -s '+driveSdxlMainFolder+' '+localModelsSdxlFolder, shell=True, env=env)\n",
        "    installSdxl(env)\n",
        "  else:\n",
        "    if not path.exists(localSdxlMainFolder):\n",
        "      os.makedirs(localSdxlMainFolder, exist_ok=True)\n",
        "    subprocess.run('ln -s '+driveSdxlMainFolder + sdxlBaseSubfolderName+' '+ localSdxlMainFolder, shell=True, env=env)\n",
        "    updateModelsYaml = True\n",
        "    with open('/content/invokeai/configs/models.yaml') as f:\n",
        "      if 'stable-diffusion-xl-base-1-0' in f.read():\n",
        "        updateModelsYaml = False\n",
        "    if updateModelsYaml:\n",
        "      with open('/content/invokeai/configs/models.yaml', 'a') as file:\n",
        "        lines = [\n",
        "          'sdxl/main/stable-diffusion-xl-base-1-0:\\n',\n",
        "          '  path: sdxl/main/stable-diffusion-xl-base-1-0\\n',\n",
        "          '  description: Stable Diffusion XL base model (12 GB)\\n',\n",
        "          '  variant: normal\\n',\n",
        "          '  format: diffusers\\n'\n",
        "        ]\n",
        "        file.writelines(lines)\n",
        "else:\n",
        "  installSdxl(env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxBRwjDuKbRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4286e6cb-d8eb-4fbc-f8aa-c39beba4868f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/invokeai\n",
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 0.462s\n",
            "34.121.155.40\n",
            "your url is: https://metal-poets-vanish.loca.lt\n",
            "2023-08-07 01:57:30.374168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "\u001b[38;20m[2023-08-07 01:57:36,515]::[InvokeAI]::INFO --> Patchmatch initialized\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,495]::[uvicorn.error]::INFO --> Started server process [14273]\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,496]::[uvicorn.error]::INFO --> Waiting for application startup.\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,496]::[InvokeAI]::INFO --> InvokeAI version 3.0.1post3\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,497]::[InvokeAI]::INFO --> Root directory = /content/invokeai\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,500]::[InvokeAI]::INFO --> GPU device = cuda Tesla T4\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:37,505]::[InvokeAI]::INFO --> Scanning /content/invokeai/models for new models\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:38,282]::[InvokeAI]::INFO --> Scanned 5 files and directories, imported 0 models\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:38,282]::[InvokeAI]::INFO --> Model manager service initialized\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:38,296]::[uvicorn.error]::INFO --> Application startup complete.\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:38,296]::[uvicorn.error]::INFO --> Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:38,842]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET / HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,001]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/index-9bb68e3a.js HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,582]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /locales/en.json HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,586]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/ThemeLocaleProvider-b9d3eb39.js HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,587]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/ThemeLocaleProvider-5b992bc7.css HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,587]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/MantineProvider-ae002ae6.js HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,769]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/favicon-0d253ced.ico HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,771]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/logo-13003d72.png HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,826]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/App-ea7b7298.js HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:39,850]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/App-6125620a.css HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:40,519]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/app/version HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,130]::[InvokeAI]::INFO --> NSFW checker initialized\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,131]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/app/config HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,135]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/models/?model_type=embedding HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,136]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/boards/?all=true HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,137]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/models/?base_models=sd-1&base_models=sd-2&base_models=sdxl&model_type=main HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,139]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/models/?model_type=vae HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,140]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/models/?model_type=controlnet HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,141]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/models/?model_type=lora HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,142]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/?board_id=none&categories=general&is_intermediate=false&limit=0&offset=0 HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,144]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/?board_id=none&categories=control&categories=mask&categories=user&categories=other&is_intermediate=false&limit=0&offset=0 HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,149]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/?board_id=none&categories=general&is_intermediate=false&limit=100&offset=0 HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:49,212]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /assets/inter-latin-wght-normal-450f3ba4.woff2 HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:50,482]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/1e2226d2-cb54-4355-a1c4-645e52918b18.png/thumbnail HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:50,488]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/1e2226d2-cb54-4355-a1c4-645e52918b18.png/metadata HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:57:50,490]::[uvicorn.access]::INFO --> 213.149.61.40:0 - \"GET /api/v1/images/1e2226d2-cb54-4355-a1c4-645e52918b18.png/full HTTP/1.1\" 200\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:58:11,418]::[uvicorn.error]::INFO --> Shutting down\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:58:11,519]::[uvicorn.error]::INFO --> Waiting for application shutdown.\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:58:11,519]::[uvicorn.error]::INFO --> Application shutdown complete.\u001b[0m\n",
            "\u001b[38;20m[2023-08-07 01:58:11,520]::[uvicorn.error]::INFO --> Finished server process [14273]\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Starting the Web UI\n",
        "\n",
        "%cd /content/invokeai/\n",
        "!npm install -g localtunnel\n",
        "\n",
        "#@markdown Copy the IP address shown in the output above the line\n",
        "#@markdown \"your url is: https://some-random-words.loca.lt\"\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "#@markdown Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"\n",
        "\n",
        "#@markdown Click the localtunnel url and paste the IP you copied earlier to the \"Endpoint IP\" text field\n",
        "!lt --port 9090 --local_https False & invokeai-web  --root /content/invokeai/\n",
        "\n",
        "#@markdown If the UI shows a red dot that says 'disconnected' when hovered in the upper\n",
        "#@markdown right corner and the Invoke button is disabled, change 'https' to 'http'\n",
        "#@markdown in the browser's address bar and press enter.\n",
        "#@markdown When the page reloads, the UI should work properly.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
