{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Invoke AI Notebook\n",
        "\n",
        "Tested: Generating images with the SDXL base model.\n",
        "\n",
        "Not tested: Using the refiner.\n",
        "\n",
        "Broken: Adding custom checkpoints."
      ],
      "metadata": {
        "id": "D4TNDJdRpPN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIhVvU8jkdm6"
      },
      "outputs": [],
      "source": [
        "#@markdown # Downloading SDLX Models and Installing InvokeAI\n",
        "\n",
        "# Models folder\n",
        "autoimportPath = '/content/invokeai/autoimport/main'\n",
        "\n",
        "#@markdown Use Google Drive to store models (uses about 13 GB). Uncheck this if you don't have enough space in your Drive.\n",
        "useGoogleDrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "googleDriveModelsFolder = '/stablemodels' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown This step usually takes about 5 minutes.\n",
        "\n",
        "from google.colab import drive\n",
        "if useGoogleDrive:\n",
        "  drive.mount('/content/drive')\n",
        "  if not googleDriveModelsFolder.startswith('/'):\n",
        "    googleDriveModelsFolder = '/' + googleDriveModelsFolder\n",
        "  modelsPath = \"/content/drive/MyDrive\"+googleDriveModelsFolder\n",
        "  if not modelsPath.endswith(\"/\"):\n",
        "   modelsPath = modelsPath + \"/\"\n",
        "\n",
        "!pip install 'InvokeAI[xformers]' --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTkFxvuH0BsX"
      },
      "outputs": [],
      "source": [
        "#@markdown # Downloading Models\n",
        "\n",
        "!mkdir /content/invokeai\n",
        "!mkdir /content/invokeai/configs\n",
        "\n",
        "#@markdown Download only the default model in initial configuration.\n",
        "#@markdown Checking this prevents running out of space in Colab.\n",
        "\n",
        "defaultOnly = True #@param {type:\"boolean\"}\n",
        "skipWeights = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown This step usually takes about 2 minutes with only the default model and no weights.\n",
        "\n",
        "#@markdown You can ignore \"File exists\" warnings in the output.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "env = os.environ.copy()\n",
        "\n",
        "cmd = 'invokeai-configure --root_dir /content/invokeai --yes'\n",
        "\n",
        "if defaultOnly:\n",
        "  cmd += ' --default_only'\n",
        "\n",
        "if skipWeights:\n",
        "  cmd += ' --skip-sd-weights'\n",
        "\n",
        "subprocess.run(cmd, shell=True, env=env)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgoAu7NF8etg"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown Adding the SDXL Base Model\n",
        "\n",
        "#@markdown Installing SDXL base took about 20 minutes initially, but it's finished instantly\n",
        "#@markdown in subsequent runs if Google Drive is enabled. You can run this in a runtime\n",
        "#@markdown without a GPU to save the model to Google Drive without spending GPU time.\n",
        "\n",
        "# If we don't have SDLX in Drive, create the folder\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "# Install the SDXL base model\n",
        "def installSdxl():\n",
        "  installCmd = 'invokeai-model-install --add \"stabilityai/stable-diffusion-xl-base-1.0\" --root_dir /content/invokeai'\n",
        "  subprocess.run(installCmd, shell=True, env=env)\n",
        "\n",
        "if useGoogleDrive:\n",
        "  alreadyInstalled = True\n",
        "\n",
        "  driveSdxlBasePath = modelsPath + \"sdxl/main\"\n",
        "  if not path.exists(driveSdxlBasePath):\n",
        "    os.makedirs(driveSdxlBasePath, exist_ok=True)\n",
        "    alreadyInstalled = False\n",
        "\n",
        "  localSdxlBaseFolder = \"/content/invokeai/models/sdxl/\"\n",
        "  localSdxlBasePath = localSdxlBaseFolder + \"main\"\n",
        "\n",
        "  subprocess.run('rm -rf ' + localSdxlBaseFolder, shell=True, env=env)\n",
        "  subprocess.run('rmdir ' + localSdxlBaseFolder, shell=True, env=env)\n",
        "\n",
        "  if not path.exists(localSdxlBaseFolder):\n",
        "    os.makedirs(localSdxlBaseFolder, exist_ok=True)\n",
        "\n",
        "  subprocess.run('ln -s '+driveSdxlBasePath+' '+localSdxlBaseFolder, shell=True, env=env)\n",
        "\n",
        "  if not alreadyInstalled:\n",
        "    installSdxl()\n",
        "  else:\n",
        "    with open(\"/content/invokeai/configs/models.yaml\", \"a\") as file:\n",
        "      lines = [\n",
        "        \"sdxl/main/stable-diffusion-xl-base-1-0:\\n\",\n",
        "        \"  path: sdxl/main/stable-diffusion-xl-base-1-0\\n\",\n",
        "        \"  description: Stable Diffusion XL base model (12 GB)\\n\",\n",
        "        \"  variant: normal\\n\",\n",
        "        \"  format: diffusers\\n\"\n",
        "      ]\n",
        "      file.writelines(lines)\n",
        "else:\n",
        "  installSdxl()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Adding the refiner and vae. This one took about 14 minutes.\n",
        "#@markdown Skip this step if you don't need these models.\n",
        "!invokeai-model-install --add \"stabilityai/stable-diffusion-xl-refiner-1.0\" --root_dir /content/invokeai\n",
        "!invokeai-model-install --add \"madebyollin/sdxl-vae-fp16-fix\" --root_dir /content/invokeai"
      ],
      "metadata": {
        "id": "aHKM_DyQirz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN-IExD5XwOs"
      },
      "outputs": [],
      "source": [
        "#@markdown # Starting the Web UI\n",
        "\n",
        "%cd /content/invokeai/\n",
        "!npm install -g localtunnel\n",
        "\n",
        "#@markdown Copy the IP address shown in the output above the line\n",
        "#@markdown \"your url is: https://some-random-words.loca.lt\"\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "#@markdown Wait for the line that says \"Uvicorn running on http://127.0.0.1:9090 (Press CTRL+C to quit)\"\n",
        "\n",
        "#@markdown Click the localtunnel url and paste the IP you copied earlier to the \"Endpoint IP\" text field\n",
        "!lt --port 9090 --local_https False & invokeai-web  --root /content/invokeai/\n",
        "\n",
        "#@markdown If the UI shows a red dot that says 'disconnected' when hovered in the upper\n",
        "#@markdown right corner and the Invoke button is disabled, change 'https' to 'http'\n",
        "#@markdown in the browser's address bar and press enter.\n",
        "#@markdown When the page reloads, the UI should work properly.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}